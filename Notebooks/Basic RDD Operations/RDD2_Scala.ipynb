{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ford GoBike's trip data\n",
    "\n",
    "https://www.fordgobike.com/system-data\n",
    "\n",
    "\n",
    "Each trip is anonymized and includes:\n",
    "- Trip Duration (seconds)\n",
    "- Start Time and Date\n",
    "- End Time and Date\n",
    "- Start Station ID\n",
    "- Start Station Name\n",
    "- Start Station Latitude\n",
    "- Start Station Longitude\n",
    "- End Station ID\n",
    "- End Station Name\n",
    "- End Station Latitude\n",
    "- End Station Longitude\n",
    "- Bike ID\n",
    "- User Type (Subscriber or Customer – “Subscriber” = Member or “Customer” = Casual)\n",
    "- Member Year of Birth\n",
    "- Member Gender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://ce7d2534405e:4040\n",
       "SparkContext available as 'sc' (version = 3.0.0, master = local[*], app id = local-1597326676544)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "rawData: org.apache.spark.rdd.RDD[String] = 2017-fordgobike-tripdata.csv MapPartitionsRDD[1] at textFile at <console>:25\n",
       "res0: Array[String] = Array(\"duration_sec\",\"start_time\",\"end_time\",\"start_station_id\",\"start_station_name\",\"start_station_latitude\",\"start_station_longitude\",\"end_station_id\",\"end_station_name\",\"end_station_latitude\",\"end_station_longitude\",\"bike_id\",\"user_type\",\"member_birth_year\",\"member_gender\", \"80110\",\"2017-12-31 16:57:39.6540\",\"2018-01-01 15:12:50.2450\",\"74\",\"Laguna St at Hayes St\",\"37.776434819204745\",\"-122.42624402046204\",\"43\",\"San Francisco Public Library (Grove St at Hyde St)\",\"37.7787677\",\"-122.4159292\",\"96\",\"Customer\",\"1987\",\"Male\")\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawData = sc.textFile(\"2017-fordgobike-tripdata.csv\")\n",
    "rawData.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration_sec\n",
      "start_time\n",
      "end_time\n",
      "start_station_id\n",
      "start_station_name\n",
      "start_station_latitude\n",
      "start_station_longitude\n",
      "end_station_id\n",
      "end_station_name\n",
      "end_station_latitude\n",
      "end_station_longitude\n",
      "bike_id\n",
      "user_type\n",
      "member_birth_year\n",
      "member_gender\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colNames: Array[String] = Array(duration_sec, start_time, end_time, start_station_id, start_station_name, start_station_latitude, start_station_longitude, end_station_id, end_station_name, end_station_latitude, end_station_longitude, bike_id, user_type, member_birth_year, member_gender)\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colNames = rawData.filter(_.startsWith(\"\\\"duration\")).flatMap(_.split(\",\")).map(x => x.slice(1, x.length -1)).collect()\n",
    "\n",
    "colNames.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > **TODO**: Utwórz RDD `transData` gdzie elementami są listy stringów (odfiltruj element zawierający nazwy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transData: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[7] at map at <console>:26\n",
       "res2: Array[String] = Array(80110, 2017-12-31 16:57:39.6540, 2018-01-01 15:12:50.2450, 74, Laguna St at Hayes St, 37.776434819204745, -122.42624402046204, 43, San Francisco Public Library (Grove St at Hyde St), 37.7787677, -122.4159292, 96, Customer, 1987, Male)\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val transData = rawData.filter(!_.startsWith(\"\\\"duration\")).map(_.split(\",\")).map(_.map(x => x.slice(1, x.length -1)))\n",
    "transData.take(1)(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Sprawdź czy, gdzie i w ilu rekordach występują braki danych w RDD `transData` (hint: elementami RDD są listy stringów)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Long = 519700\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transData.map(_.contains(\"\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Uwzględniając informacje uzyskane w poprzednim ćwiczeniu utwórz RDD `data` którego elementy będą słownikami. Słowniki te mają zawierać klucze o wartościach następujących typów:\n",
    "- 'duration_sec' : \"int\"\n",
    "- 'start_time' : \"datetime\"\n",
    "- 'end_time' : \"datetime\"\n",
    "- 'start_station_id' : \"int\"\n",
    "- 'start_station_name' : \"string\"\n",
    "- 'start_station_latitude' : \"float\"\n",
    "- 'start_station_longitude' : \"float\"\n",
    "- 'end_station_id' : \"int\"\n",
    "- 'end_station_name' : \"string\"\n",
    "- 'end_station_latitude' : \"float\"\n",
    "- 'end_station_longitude' : \"float\"\n",
    "- 'bike_id' : \"int\"\n",
    "- 'user_type' : \"string\"\n",
    "- 'member_birth_year' : \"int\"\n",
    "- 'member_gender' : \"string\"\n",
    "\n",
    "> (hint: autor zadania rozwiązał je pisząc funkcję przekształcającą elementy w RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "struct: Array[String] = Array(Int, Datetime, Datetime, Int, String, Float, Float, Int, String, Float, Float, Int, String, Int, String)\n",
       "res4: String =\n",
       "Int\n",
       "Datetime\n",
       "Datetime\n",
       "Int\n",
       "String\n",
       "Float\n",
       "Float\n",
       "Int\n",
       "String\n",
       "Float\n",
       "Float\n",
       "Int\n",
       "String\n",
       "Int\n",
       "String\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val struct = Array(\"Int\", \n",
    "          \"Datetime\", \n",
    "          \"Datetime\", \n",
    "          \"Int\", \n",
    "          \"String\", \n",
    "          \"Float\", \n",
    "          \"Float\", \n",
    "          \"Int\", \n",
    "          \"String\", \n",
    "          \"Float\", \n",
    "          \"Float\", \n",
    "          \"Int\", \n",
    "          \"String\", \n",
    "          \"Int\", \n",
    "          \"String\")\n",
    "\n",
    "struct.mkString(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.time.LocalDateTime\n",
       "import java.time.format.DateTimeFormatter\n",
       "import java.time.temporal.ChronoUnit\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  import java.time.LocalDateTime\n",
    "  import java.time.format.DateTimeFormatter\n",
    "  import java.time.temporal.ChronoUnit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zipMany: [X, Y, Z](xs: Seq[X], ys: Seq[Y], zs: Seq[Z])Seq[(X, Y, Z)]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  def zipMany[X,Y,Z](xs: Seq[X], ys: Seq[Y], zs: Seq[Z]): Seq[(X,Y,Z)] =\n",
    "    for (((x, y), z) <- xs zip ys zip zs) yield (x, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transform_element: (rdd_elem: Array[String], elem_struct: Array[String], names: Array[String])scala.collection.mutable.Map[String,Any]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def transform_element (rdd_elem: Array[String], elem_struct: Array[String], names: Array[String]) = {\n",
    "    val DATE_FORMATTER = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSSS\")\n",
    "    var map = scala.collection.mutable.Map[String, Any]()\n",
    "\n",
    "    val zipped = zipMany(rdd_elem, elem_struct, names)\n",
    "\n",
    "    for ((elem, e_type, name) <- zipped) {\n",
    "      e_type match  {\n",
    "        case \"Int\" => elem match {\n",
    "          case \"\" => map += (name -> 9999)\n",
    "          case _ => map += (name -> elem.toInt)\n",
    "        }\n",
    "        case \"Datetime\" => map += (name -> LocalDateTime.parse(elem, DATE_FORMATTER))\n",
    "        case \"Float\" => map += (name -> elem.toFloat)\n",
    "        case _ => map += (name -> elem)\n",
    "      }\n",
    "    }\n",
    "    map\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.rdd.RDD[scala.collection.mutable.Map[String,Any]] = MapPartitionsRDD[9] at map at <console>:35\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val data = transData.map(x => transform_element(x, struct, colNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: String =\n",
       "start_station_id -> 74\n",
       "member_gender -> Male\n",
       "start_station_longitude -> -122.42625\n",
       "end_station_latitude -> 37.778767\n",
       "member_birth_year -> 1987\n",
       "start_station_name -> Laguna St at Hayes St\n",
       "end_station_longitude -> -122.41593\n",
       "start_time -> 2017-12-31T16:57:39.654\n",
       "end_time -> 2018-01-01T15:12:50.245\n",
       "duration_sec -> 80110\n",
       "user_type -> Customer\n",
       "end_station_name -> San Francisco Public Library (Grove St at Hyde St)\n",
       "start_station_latitude -> 37.776436\n",
       "bike_id -> 96\n",
       "end_station_id -> 43\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(1)(0).mkString(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: data.type = MapPartitionsRDD[9] at map at <console>:35\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ZADANIE 1**: Zapoznaj się z danymi:\n",
    "1. Sprawdź rozkład zmiennej \"member_gender\"\n",
    "2. Oblicz minimalny, maksymalny i średni wiek osób wypożyczających rowery\n",
    "3. Oblicz liczbę unikalnych rowerów\n",
    "4. Oblicz liczbę unikalnych stacji\n",
    "5. Sprawdź który rower był wypożyczony najdłużej a który najkrócej w ciągu analizowanego okresu (oraz jak długo)\n",
    "6. Oblicz średni czas pojedynczego wypożyczenia\n",
    "7. Sprawdź pomiędzy którymi stacjami występował największy ruch\n",
    "8. Sprawdź o której godzinie w ciągu dnia wypożyczano najwięcej rowerów\n",
    "9. Sprawdź *średnią liczbę wypożyczeń* dla poszczególnych dni tygodnia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "member_gender: Array[(Any, Int)] = Array((Other,6299), (\"\",66462), (Female,98621), (Male,348318))\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//1\n",
    "val member_gender =  data.map(x => (x(\"member_gender\"),1)).reduceByKey(_ + _).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min_year: org.apache.spark.util.StatCounter = (count: 453159, mean: 39.595213, stdev: 10.513476, max: 134.000000, min: 21.000000)\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//2\n",
    "val min_year = data.filter(x => x(\"member_birth_year\") != 9999\n",
    "                          ).map(x => 2020 - x(\"member_birth_year\").asInstanceOf[Int]).stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Long = 3673\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//3\n",
    "data.map(_(\"bike_id\")).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Long = 272\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//4\n",
    "data.map(_(\"start_station_id\")).union(data.map(_(\"end_station_id\"))).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bike_rental_max: (Any, Int) = (775,555010)\n",
       "bike_rental_min: (Any, Int) = (722,70)\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//5\n",
    "val bike_rental_max = data.map(x => (x(\"bike_id\"), x(\"duration_sec\").asInstanceOf[Int])\n",
    "                              ).reduceByKey(_ + _).max()(Ordering.by[(Any, Int), Int](_._2))\n",
    "\n",
    "val bike_rental_min = data.map(x => (x(\"bike_id\"), x(\"duration_sec\").asInstanceOf[Int])\n",
    "                              ).reduceByKey(_ + _).min()(Ordering.by[(Any, Int), Int](_._2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avg_rental: Double = 1099.0095208774264\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//6\n",
    "val avg_rental = data.map(_(\"duration_sec\").asInstanceOf[Int]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "most_popular_stations: Array[(String, Int)] = Array((15-6,5078), (16-6,3154), (81-15,3087))\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//7\n",
    "val most_popular_stations = data.map{\n",
    "  case x if x(\"start_station_id\").asInstanceOf[Int] > x(\"end_station_id\").asInstanceOf[Int] => (x(\"start_station_id\") + \"-\" + x(\"end_station_id\"), 1)\n",
    "  case x => (x(\"end_station_id\") + \"-\" + x(\"start_station_id\"), 1)\n",
    "}.reduceByKey(_ + _).takeOrdered(3)(Ordering[Int].reverse.on(_._2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rush_hour: Array[(Int, Int)] = Array((4,640), (16,41339), (0,2606), (8,57227), (12,27448), (20,16642), (13,27374), (21,11993), (1,1377), (17,60111), (9,45457), (5,2145), (22,7967), (14,24900), (6,8895), (18,46088), (10,25217), (2,860), (19,26598), (15,28448), (11,24303), (23,4867), (3,398), (7,26800))\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//8\n",
    "val rush_hour = data.map(x => (x(\"start_time\").asInstanceOf[java.time.LocalDateTime].getHour, 1)\n",
    "                        ).reduceByKey(_ + _).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(MONDAY,3131)\n",
      "(TUESDAY,3379)\n",
      "(WEDNESDAY,3250)\n",
      "(THURSDAY,3157)\n",
      "(FRIDAY,3006)\n",
      "(SATURDAY,1884)\n",
      "(SUNDAY,1681)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "intiaValue: (Int, Int) = (0,0)\n",
       "counter: ((Int, Int), Int) => (Int, Int) = $Lambda$3166/0x000000084112d040@7ac344e0\n",
       "combCounter: ((Int, Int), (Int, Int)) => (Int, Int) = $Lambda$3167/0x000000084112d840@3e61903b\n",
       "rush_days: Array[(java.time.DayOfWeek, Int)] = Array((MONDAY,3131), (TUESDAY,3379), (WEDNESDAY,3250), (THURSDAY,3157), (FRIDAY,3006), (SATURDAY,1884), (SUNDAY,1681))\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//9\n",
    "  val intiaValue = (0, 0)\n",
    "  val counter = (acc: (Int, Int), x: Int) => (acc._1 + x, acc._2 +1)\n",
    "  val combCounter = (acc1: (Int, Int), acc2: (Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n",
    "\n",
    "  val rush_days = data.map(x => (x(\"start_time\").asInstanceOf[java.time.LocalDateTime].toLocalDate, 1)\n",
    "  ).reduceByKey(_ + _ ).map(x => (x._1.getDayOfWeek, x._2)\n",
    "  ).aggregateByKey(intiaValue)(counter, combCounter\n",
    "  ).mapValues(x => x._1 / x._2).takeOrdered(7)\n",
    "\n",
    "rush_days.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ZADANIE 2**: Utwórz RDD `dataDaily` zawierające dane zagregowane do poziomu dnia. Każdy dzień w roku (element w RDD) ma zawierać następujące informacje: \n",
    "- 'date' : data \n",
    "- 'avg_duration_sec' : średni czas wypożyczeń danego dnia\n",
    "- 'n_trips' : liczba wypożyczeń danego dnia\n",
    "- 'n_bikes' : liczba unikatowych rowerów użytych danego dnia\n",
    "- 'n_routes' : liczba unikatowych kombinacji stacji (x -> y == y -> x) danego dnia\n",
    "- 'n_subscriber' : liczba wypożyczeń dokonanych przez subskrybentów danego dnia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avg_time_trip: org.apache.spark.rdd.RDD[(java.time.LocalDate, (Int, Int))] = MapPartitionsRDD[46] at mapValues at <console>:36\n",
       "res10: Array[(java.time.LocalDate, (Int, Int))] = Array((2017-08-12,(1671,1945)), (2017-07-08,(2088,659)), (2017-11-12,(1528,2075)), (2017-11-20,(826,4049)), (2017-12-24,(1260,819)))\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//avg_duration and n_trips\n",
    "val avg_time_trip = data.map(x => (x(\"start_time\").asInstanceOf[LocalDateTime].toLocalDate, x(\"duration_sec\").asInstanceOf[Int])\n",
    "  ).aggregateByKey(intiaValue)(counter, combCounter\n",
    "  ).mapValues(x => (x._1 /x._2, x._2))\n",
    "\n",
    "avg_time_trip.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uniq_bikes: org.apache.spark.rdd.RDD[(java.time.LocalDate, Int)] = ShuffledRDD[52] at reduceByKey at <console>:31\n",
       "res11: Array[(java.time.LocalDate, Int)] = Array((2017-07-08,305), (2017-08-12,937), (2017-07-24,752), (2017-10-16,1397), (2017-10-20,1310))\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " //number of unique bikes\n",
    "val uniq_bikes = data.map(x => (x(\"start_time\").asInstanceOf[LocalDateTime].toLocalDate, x(\"bike_id\"))).distinct(\n",
    ").mapValues(x => 1).reduceByKey(_ + _)\n",
    "\n",
    "uniq_bikes.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "routes: org.apache.spark.rdd.RDD[(java.time.LocalDate, Int)] = ShuffledRDD[58] at reduceByKey at <console>:33\n",
       "res12: Array[(java.time.LocalDate, Int)] = Array((2017-07-08,276), (2017-08-12,933), (2017-07-24,826), (2017-10-16,1974), (2017-10-20,1883))\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  //number of unique station\n",
    "  val routes = data.map{\n",
    "    case x if x(\"start_station_id\").asInstanceOf[Int] > x(\"end_station_id\").asInstanceOf[Int] => (x(\"start_time\").asInstanceOf[LocalDateTime].toLocalDate, x(\"start_station_id\") + \"-\" + x(\"end_station_id\"))\n",
    "    case x => (x(\"start_time\").asInstanceOf[LocalDateTime].toLocalDate, x(\"end_station_id\") + \"-\" + x(\"start_station_id\"))\n",
    "  }.distinct().mapValues(x => 1).reduceByKey(_ + _)\n",
    "\n",
    "routes.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subscribers: org.apache.spark.rdd.RDD[(java.time.LocalDate, Int)] = ShuffledRDD[61] at reduceByKey at <console>:31\n",
       "res13: Array[(java.time.LocalDate, Int)] = Array((2017-08-12,813), (2017-07-08,281), (2017-11-12,1335), (2017-11-20,3540), (2017-12-24,573))\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  //number of subscribers\n",
    "  val subscribers = data.filter(_(\"user_type\") == \"Subscriber\").map(x =>(x(\"start_time\").asInstanceOf[LocalDateTime].toLocalDate, 1)\n",
    "  ).reduceByKey(_ + _)\n",
    "\n",
    "subscribers.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataDaily: org.apache.spark.rdd.RDD[(java.time.LocalDate, (Int, Int, Int, Int, Int))] = MapPartitionsRDD[73] at mapValues at <console>:37\n",
       "res14: String = (2017-08-12,(1671,1945,937,933,813))\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataDaily = avg_time_trip.join(uniq_bikes).mapValues(x => (x._1._1, x._1._2, x._2)\n",
    "                            ).join(routes).mapValues(x => (x._1._1, x._1._2, x._1._3, x._2)\n",
    "                            ).join(subscribers).mapValues(x => (x._1._1, x._1._2, x._1._3, x._1._4, x._2))\n",
    "\n",
    "dataDaily.take(1).mkString(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "daily: org.apache.spark.rdd.RDD[scala.collection.immutable.Map[String,Any]] = MapPartitionsRDD[74] at map at <console>:29\n",
       "res15: String =\n",
       "Map(n_routes -> 933, date -> 2017-08-12, n_trips -> 1945, n_bikes -> 937, n_subscribers -> 813, avg_duration_sec -> 1671)\n",
       "Map(n_routes -> 276, date -> 2017-07-08, n_trips -> 659, n_bikes -> 305, n_subscribers -> 281, avg_duration_sec -> 2088)\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  val daily = dataDaily.map(x => Map(\n",
    "    \"date\" -> x._1,\n",
    "    \"avg_duration_sec\" -> x._2._1,\n",
    "    \"n_trips\" -> x._2._2,\n",
    "    \"n_bikes\" -> x._2._3,\n",
    "    \"n_routes\" -> x._2._4,\n",
    "    \"n_subscribers\" -> x._2._5\n",
    "  ))\n",
    "\n",
    "daily.take(2).mkString(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.time.LocalDate\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.time.LocalDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map(n_routes -> 1605, date -> 2017-12-21, n_trips -> 2995, n_bikes -> 1321, n_subscribers -> 2724, avg_duration_sec -> 743)\n"
     ]
    }
   ],
   "source": [
    "daily.filter(_(\"date\") == java.time.LocalDate.of(2017, 12, 21)).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **⋆ ZADANIE 3**: Utwórz RDD klucz-wartość `bikeDaily` zawierające po jednym elemencie dla każdego `\"bike_id\"`. Wartościami w RDD mają być listy całkowitego dziennego wykorzystania danego roweru w sekundach (elementy listy uszeregowane chronologicznie). \n",
    "- kryterium zaliczenia danego wypożyczenia do konkretnego dnia jest moment startu\n",
    "- wszystkie rowery mają mieć listy jednakowej długości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test: org.apache.spark.rdd.RDD[((Any, java.time.LocalDate), Int)] = ShuffledRDD[77] at reduceByKey at <console>:32\n",
       "res17: Array[((Any, java.time.LocalDate), Int)] = Array(((1172,2017-09-17),504))\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test = data.map(x => ((x(\"bike_id\"), x(\"start_time\").asInstanceOf[LocalDateTime].toLocalDate), \n",
    "                          x(\"duration_sec\").asInstanceOf[Int])\n",
    "            ).reduceByKey(_ + _)\n",
    "\n",
    "test.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first_day: Long = 17345\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val first_day = data.map(x => x(\"start_time\").asInstanceOf[LocalDateTime].toLocalDate.toEpochDay).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min_date: java.time.LocalDate = 2017-06-28\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val min_date = LocalDate.ofEpochDay(first_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "last_day: Long = 17531\n",
       "time_delta: Long = 186\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val last_day = data.map(x => x(\"start_time\").asInstanceOf[LocalDateTime].toLocalDate.toEpochDay).max()\n",
    "val time_delta = last_day - first_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date_list: scala.collection.immutable.IndexedSeq[java.time.LocalDate] = Vector(2017-06-28, 2017-06-29, 2017-06-30, 2017-07-01, 2017-07-02, 2017-07-03, 2017-07-04, 2017-07-05, 2017-07-06, 2017-07-07, 2017-07-08, 2017-07-09, 2017-07-10, 2017-07-11, 2017-07-12, 2017-07-13, 2017-07-14, 2017-07-15, 2017-07-16, 2017-07-17, 2017-07-18, 2017-07-19, 2017-07-20, 2017-07-21, 2017-07-22, 2017-07-23, 2017-07-24, 2017-07-25, 2017-07-26, 2017-07-27, 2017-07-28, 2017-07-29, 2017-07-30, 2017-07-31, 2017-08-01, 2017-08-02, 2017-08-03, 2017-08-04, 2017-08-05, 2017-08-06, 2017-08-07, 2017-08-08, 2017-08-09, 2017-08-10, 2017-08-11, 2017-08-12, 2017-08-13, 2017-08-14, 2017-08-15, 2017-08-16, 2017-08-17, 2017-08-18, 2017-08-19, 2017-08-20, 2017-08-21, 2017-08-22, 2017-08-23, 2017-08-24, 2017-08-25, 2017-08-26...\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val date_list = (0 to time_delta.toInt).map(x => min_date.plusDays(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bike_list: Array[Any] = Array(1084, 772, 3272, 752, 1724, 428, 1900, 1328, 464, 1040, 1336, 912, 140, 204, 956, 1128, 2892, 3436, 228, 1416, 192, 492, 160, 1148, 1168, 1596, 1780, 2716, 1304, 3192, 992, 2904, 3240, 1500, 460, 2664, 2608, 3336, 548, 1404, 1732, 2092, 528, 696, 2108, 1632, 3032, 1716, 1308, 1808, 2712, 2912, 1280, 2052, 2116, 1816, 196, 2192, 1104, 784, 2732, 848, 3296, 3568, 1144, 1176, 292, 1592, 1296, 2180, 2164, 344, 1344, 1320, 664, 252, 2604, 2464, 1020, 3500, 2040, 988, 2360, 688, 2600, 968, 2964, 592, 1468, 1940, 836, 1556, 2924, 1800, 1932, 3484, 1656, 2744, 740, 436, 208, 2808, 3700, 3516, 3600, 1968, 804, 2792, 2768, 3520, 3000, 1172, 3616, 3128, 2588, 1424, 1044, 1076, 536, 2648, 1100, 3364, 3696, 2488, 3488, 3180, 2844, 1300, 2976, 3452, 672, 2248, 1540, 1680...\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bike_list = data.map(_(\"bike_id\")).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comb_list: Array[((Any, java.time.LocalDate), Int)] = Array(((1084,2017-06-28),0), ((1084,2017-06-29),0), ((1084,2017-06-30),0), ((1084,2017-07-01),0), ((1084,2017-07-02),0), ((1084,2017-07-03),0), ((1084,2017-07-04),0), ((1084,2017-07-05),0), ((1084,2017-07-06),0), ((1084,2017-07-07),0), ((1084,2017-07-08),0), ((1084,2017-07-09),0), ((1084,2017-07-10),0), ((1084,2017-07-11),0), ((1084,2017-07-12),0), ((1084,2017-07-13),0), ((1084,2017-07-14),0), ((1084,2017-07-15),0), ((1084,2017-07-16),0), ((1084,2017-07-17),0), ((1084,2017-07-18),0), ((1084,2017-07-19),0), ((1084,2017-07-20),0), ((1084,2017-07-21),0), ((1084,2017-07-22),0), ((1084,2017-07-23),0), ((1084,2017-07-24),0), ((1084,2017-07-25),0), ((1084,2017-07-26),0), ((1084,2017-07-27),0), ((1084,2017-07-28),0), ((1084,2017-07-29),0), (...\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val comb_list = for{ bike <- bike_list; date <- date_list} yield ((bike, date), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "helper_rdd: org.apache.spark.rdd.RDD[((Any, java.time.LocalDate), Int)] = ParallelCollectionRDD[89] at parallelize at <console>:31\n",
       "res22: Array[((Any, java.time.LocalDate), Int)] = Array(((1084,2017-06-28),0), ((1084,2017-06-29),0))\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val helper_rdd = sc.parallelize(comb_list)\n",
    "helper_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temp_rdd: org.apache.spark.rdd.RDD[(Any, (java.time.LocalDate, Int))] = MapPartitionsRDD[93] at map at <console>:34\n",
       "res24: Array[(Any, (java.time.LocalDate, Int))] = Array((1172,(2017-09-17,504)), (1829,(2017-09-14,2978)))\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val temp_rdd = test.union(helper_rdd).map(x => (x._1._1,(x._1._2, x._2)))\n",
    "temp_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res29: Array[(Any, List[(java.time.LocalDate, Int)])] = Array((3272,List((2017-06-28,0), (2017-06-29,0), (2017-06-30,0), (2017-07-01,0), (2017-07-02,0), (2017-07-03,0), (2017-07-04,0), (2017-07-05,0), (2017-07-06,0), (2017-07-07,0), (2017-07-08,0), (2017-07-09,0), (2017-07-10,0), (2017-07-11,0), (2017-07-12,0), (2017-07-13,0), (2017-07-14,0), (2017-07-15,0), (2017-07-16,0), (2017-07-17,0), (2017-07-18,0), (2017-07-19,0), (2017-07-20,0), (2017-07-21,0), (2017-07-22,0), (2017-07-23,0), (2017-07-24,0), (2017-07-25,0), (2017-07-26,0), (2017-07-27,0), (2017-07-28,0), (2017-07-29,0), (2017-07-30,0), (2017-07-31,0), (2017-08-01,0), (2017-08-02,0), (2017-08-03,0), (2017-08-04,0), (2017-08-05,0), (2017-08-06,0), (2017-08-07,0), (2017-08-08,0), (2017-08-09,0), (2017-08-10,0), (2017-08-11,0), (201...\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_rdd.groupByKey().mapValues(x => x.toList.sortBy(_._1.toEpochDay)).collect()                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przetwarzanie tekstu\n",
    "[Project Gutenberg](http://www.gutenberg.org/)\n",
    "> **ZADANIE 4**: Wstępnie przetwórz i zbadaj Moby Dicka\n",
    "1. usuń puste linie, tytuł i nazwy rozdziałów, wynik zapisz jako `textOnlyMD`\n",
    "2. policz ile słów znajduje się w tekście\n",
    "3. policz ile unikalnych słów występuje w tekście\n",
    "4. znajdź 10 najczęstszych słów\n",
    "5. sprawdź jak często wystepowało slowo \"whale\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text: org.apache.spark.rdd.RDD[String] = MobyDick.txt MapPartitionsRDD[7] at textFile at <console>:27\n",
       "res3: Array[String] = Array(MOBY DICK;, \"\", \"\", or, THE WHALE., \"\", \"\", CHAPTER 1. Loomings., \"\", Call me Ishmael. Some years ago—never mind how long precisely—having, little or no money in my purse, and nothing particular to interest me on)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val text = sc.textFile(\"MobyDick.txt\")\n",
    "text.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOBY DICK;\n",
      "\n",
      "\n",
      "or, THE WHALE.\n",
      "\n",
      "\n",
      "CHAPTER 1. Loomings.\n",
      "\n",
      "Call me Ishmael. Some years ago—never mind how long precisely—having\n",
      "little or no money in my purse, and nothing particular to interest me on\n"
     ]
    }
   ],
   "source": [
    "text.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "header: scala.collection.immutable.Set[String] = Set(MOBY DICK;, \"\", or, THE WHALE.)\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val header = text.take(6).toSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER 1. Loomings.\n",
      "Call me Ishmael. Some years ago—never mind how long precisely—having\n",
      "little or no money in my purse, and nothing particular to interest me on\n",
      "shore, I thought I would sail about a little and see the watery part of\n",
      "the world. It is a way I have of driving off the spleen and regulating\n",
      "the circulation. Whenever I find myself growing grim about the mouth;\n",
      "whenever it is a damp, drizzly November in my soul; whenever I find\n",
      "myself involuntarily pausing before coffin warehouses, and bringing up\n",
      "the rear of every funeral I meet; and especially whenever my hypos get\n",
      "such an upper hand of me, that it requires a strong moral principle to\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "textOnlyMD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[11] at filter at <console>:31\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//1\n",
    "val textOnlyMD = text.filter(!header.contains(_))\n",
    "\n",
    "textOnlyMD.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Long = 219137\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//2\n",
    "textOnlyMD.flatMap(_.split(\"\\\\W+\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.util.matching.Regex\n",
       "pattern: scala.util.matching.Regex = [a-zA-Z]+\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//2 other approach by regex\n",
    "import scala.util.matching.Regex\n",
    "\n",
    "val pattern = new Regex(\"[a-zA-Z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Long = 217676\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textOnlyMD.flatMap(x => pattern.findAllIn(x.toLowerCase)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: Long = 16888\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//3\n",
    "textOnlyMD.flatMap(x => pattern.findAllIn(x.toLowerCase)).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(the,14348)\n",
      "(of,6583)\n",
      "(and,6393)\n",
      "(a,4695)\n",
      "(to,4619)\n",
      "(in,4135)\n",
      "(that,3060)\n",
      "(it,2508)\n",
      "(his,2495)\n",
      "(i,2114)\n"
     ]
    }
   ],
   "source": [
    "//4\n",
    "textOnlyMD.flatMap(x => pattern.findAllIn(x.toLowerCase)).map(x => (x,1)).reduceByKey(_ + _\n",
    ").takeOrdered(10)(Ordering[Int].reverse.on(_._2)).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res26: Long = 1152\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//5 occurance of word whale\n",
    "textOnlyMD.flatMap(x => pattern.findAllIn(x.toLowerCase)).filter(_ == \"whale\").count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
