{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RDD OPERATIONS - map, flatMap etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddlist: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[16] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddlist = sc.parallelize(0 to 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[Int] = Array(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddlist.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddText: org.apache.spark.rdd.RDD[String] = RDD.txt MapPartitionsRDD[2] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddText = sc.textFile(\"RDD.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[String] = Array(0,1,2,3,4,5,6,7,8,9)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddText.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddlist2: org.apache.spark.rdd.RDD[List[Int]] = ParallelCollectionRDD[7] at parallelize at <console>:27\n",
       "res14: Array[List[Int]] = Array(List(0, 1, 2, 3, 4), List(5, 6, 7, 8, 9))\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddlist2 = sc.parallelize(List((0 to 4).toList, (5 to 9).toList))\n",
    "rddlist2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddlist2.flatMap(x => x.map(_ + 1)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: W RDDlist2 zastąp liczby parzyste literą *P* a nieparzyste *N*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Array[List[String]] = Array(List(P, N, P, N, P), List(N, P, N, P, N))\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddlist2.map(x => x.map{case x if x % 2 == 0 => \"P\"; case _ => \"N\"}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res22: Array[Int] = Array(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddlist2.flatMap(x => x.map(x  => x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mapValues(func) \n",
    "Example from lecture -> [(\"A\",[\"Adam\",\"Ada\",\"Adrian\"]),(\"B\",[\"Bonifacy\",\"Barnaba\"])]\n",
    "\n",
    "Note! mapValues deprecated in Scala, therefore regular map used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mapa: List[(String, List[String])] = List((A,List(Adam, Ada, Adrian)), (B,List(Bonifacy, Barnaba)))\n",
       "rddpair: org.apache.spark.rdd.RDD[(String, List[String])] = ParallelCollectionRDD[0] at parallelize at <console>:26\n",
       "res1: Array[(String, List[String])] = Array((A,List(Adam, Ada, Adrian)), (B,List(Bonifacy, Barnaba)))\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mapa = List((\"A\", List(\"Adam\", \"Ada\", \"Adrian\")), (\"B\", List(\"Bonifacy\", \"Barnaba\")))\n",
    "val rddpair = sc.parallelize(mapa)\n",
    "\n",
    "rddpair.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return tuple of key and length of of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res27: Array[(String, Int)] = Array((A,3), (B,2))\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddpair.map { case (k,v) => (k, v.length)}.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Zmodyfikuj wartości w RDDpair tak aby zawierały nie imiona a liczby liter w imionach, następnie zsumuj liczby liter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res28: Array[(String, List[Int])] = Array((A,List(4, 3, 6)), (B,List(8, 7)))\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddpair.map { case (k,v) => (k, v.map(y => y.length))}.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res29: Array[(String, Int)] = Array((A,13), (B,15))\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddpair.map { case (k,v) => (k, v.map(y => y.length).sum)}.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Na podstawie RDDpair stwórz RDD o następującej strukturze: (litera, (imię, liczba liter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A,List((Adam,4), (Ada,3), (Adrian,6)))\n",
      "(B,List((Bonifacy,8), (Barnaba,7)))\n"
     ]
    }
   ],
   "source": [
    "rddpair.map { case (k,v) => (k, v.map(x => (x, x.length)) )}.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### join(RDD), union, distinct, groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A,(Adam,4))\n",
      "(A,(Ada,3))\n",
      "(A,(Adrian,6))\n",
      "(B,(Bonifacy,8))\n",
      "(B,(Barnaba,7))\n"
     ]
    }
   ],
   "source": [
    "rddpair.flatMapValues(x => x).map{ case (k,v) => (k,(v,v.length))}.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A,(List(Adam, Ada, Adrian),Adam))\n",
      "(A,(List(Adam, Ada, Adrian),Ada))\n",
      "(A,(List(Adam, Ada, Adrian),Adrian))\n",
      "(B,(List(Bonifacy, Barnaba),Bonifacy))\n",
      "(B,(List(Bonifacy, Barnaba),Barnaba))\n"
     ]
    }
   ],
   "source": [
    "rddpair.join(rddpair.flatMapValues(x => x)).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: Array[(Int, List[Int])] = Array((0,List(0, 2, 4, 6, 8, 10)), (1,List(1, 3, 5, 7, 9)))\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddlist.groupBy(_ % 2).mapValues(_.toList).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "names: List[(String, Int)] = List((Adam,4), (Ada,3), (Adrian,6), (Bonifacy,8), (Barnaba,7))\n",
       "rddpair2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[22] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val names = List((\"Adam\", 4), (\"Ada\", 3), (\"Adrian\", 6), (\"Bonifacy\", 8), (\"Barnaba\", 7))\n",
    "val rddpair2 = sc.parallelize(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: Array[(String, Iterable[(String, Int)])] = Array((Even,CompactBuffer((Adam,4), (Adrian,6), (Bonifacy,8))), (Odd,CompactBuffer((Ada,3), (Barnaba,7))))\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddpair2.groupBy{ case (x,y) if y % 2 == 0 => \"Even\"; case _ => \"Odd\"}.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Pogrupuj RDDpair2 ze względu na pierwszą literę imienia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res22: Array[(Char, Iterable[(String, Int)])] = Array((A,CompactBuffer((Adam,4), (Ada,3), (Adrian,6))), (B,CompactBuffer((Bonifacy,8), (Barnaba,7))))\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddpair2.groupBy(_._1.charAt(0)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### groupByKey(), reduceByKey(func), aggregateByKey(zeroValue, seqFunc, combFunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO**: Na podstawie RDDpair stwórz RDD o następującej strukturze: (litera, (imię, l. liter)), następnie pogrupuj je po literze (zamień nowe wartości na listę tak aby były czytelne po użyciu `collect`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A,List((Adam,4), (Ada,3), (Adrian,6)))\n",
      "(B,List((Bonifacy,8), (Barnaba,7)))\n"
     ]
    }
   ],
   "source": [
    "rddpair.flatMapValues(x => x).map{ case (k,v) => (k,(v,v.length))}.groupByKey.mapValues(_.toList)\n",
    ".collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddpair3: Array[(Char, Int)] = Array((A,4), (A,3), (A,6), (B,8), (B,7))\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddpair3 = rddpair2.groupBy(_._1.charAt(0)).values.map(_.toList).flatMap(x=> x).map(x=> (x._1.charAt(0), x._2))\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">##### **TODO**: Uzyskaj iloczyn dla każdego klucza w RDDpair3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res39: scala.collection.immutable.Map[Char,Int] = Map(A -> 72, B -> 56)\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddpair3.groupBy(_._1).map{case (k,v) => (k, v.map(_._2).product)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddpair4: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[0] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddpair4 = sc.parallelize(List((\"A\", \"Adam\"),(\"A\", \"Ada\"),(\"A\", \"Adrian\"),(\"B\", \"Bonifacy\"),(\"B\", \"Barnaba\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozbudowana wersja `reduceByKey` pozwalająca na zwrócenie wartości o innym typie niż oryginalne. W związku z tym konieczne jest podanie trzech parametrów:\n",
    "- zeroValue - domyślna wartość neutralna dla agregacji (dodawanie: 0, mnożenie: 1, tworzenie zbioru unikatowych wartości: pusty zbiór, itd.),\n",
    "- seqFunc - funkcja agregująca wartości w oryginalnym RDD, przyjmuje dwa parametry, gdzie drugi jest włączany (dodawany itp.) do pierwszego\n",
    "- combFunc - funkcja łącząca wartości uzyskane z pierwszej funkcji dla kluczy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initiaValue: Int = 0\n",
       "addToCounts: (Int, String) => Int = $Lambda$2278/0x00000008409f5040@2fa16cb3\n",
       "sumPartitionCounts: (Int, Int) => Int = $Lambda$2279/0x0000000840502040@41e2ed85\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val initiaValue = 0\n",
    "val addToCounts = (n:Int, x: String) => n + x.length\n",
    "val sumPartitionCounts = (p1: Int, p2: Int) => p1 + p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[(String, Int)] = Array((A,13), (B,15))\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddpair4.aggregateByKey(initiaValue)(addToCounts, sumPartitionCounts).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
