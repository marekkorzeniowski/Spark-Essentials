{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://ce7d2534405e:4040\n",
       "SparkContext available as 'sc' (version = 3.0.0, master = local[*], app id = local-1597569480518)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "people: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//json\n",
    "val people = spark.read.json(\"people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|  35|   Emma|\n",
      "|null|   null|\n",
      "|  31|   null|\n",
      "|  30|Michael|\n",
      "|  19|    Eva|\n",
      "|null|   Emma|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   name| _c1|\n",
      "+-------+----+\n",
      "|Michael|29.0|\n",
      "|   Andy|30.0|\n",
      "| Justin|19.0|\n",
      "+-------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "people_txt: org.apache.spark.sql.DataFrame = [name: string, _c1: double]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//csv\n",
    "val people_txt = spark.read.option(\"inferSchema\", \"true\").csv(\"people.txt\").select(col(\"_c0\").as(\"name\"), col(\"_c1\"))\n",
    "people_txt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "import org.apache.spark.sql.types._\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// import org.apache.spark.sql._\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "person1: Seq[org.apache.spark.sql.Row] = List([Greg,32], [ann,12], [Marek,26])\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val person1 = Seq(Row(\"Greg\", 32), Row(\"ann\", 12), Row(\"Marek\", 26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "someSchema: List[org.apache.spark.sql.types.StructField] = List(StructField(name,StringType,true), StructField(age,IntegerType,true))\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val someSchema = List(\n",
    "    StructField(\"name\", StringType, true),\n",
    "    StructField(\"age\", IntegerType, true)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [name: string, age: int]\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.createDataFrame(spark.sparkContext.parallelize(person1),\n",
    "  StructType(someSchema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "| Greg| 32|\n",
      "|  ann| 12|\n",
      "|Marek| 26|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "random: scala.util.Random = scala.util.Random@4c4a9898\n",
       "years: Int = 6\n",
       "names: List[String] = List(Alice, Betty, Chris, Dan, Greg)\n",
       "names_count: Int = 5\n",
       "names_list: scala.collection.immutable.IndexedSeq[String] = Vector(Alice, Alice, Alice, Alice, Alice, Alice, Betty, Betty, Betty, Betty, Betty, Betty, Chris, Chris, Chris, Chris, Chris, Chris, Dan, Dan, Dan, Dan, Dan, Dan, Greg, Greg, Greg, Greg, Greg, Greg)\n",
       "year: scala.collection.immutable.IndexedSeq[Int] = Vector(2005, 2006, 2007, 2008, 2009, 2010, 2005, 2006, 2007, 2008, 2009, 2010, 2005, 2006, 2007, 2008, 2009, 2010, 2005, 2006, 2007, 2008, 2009, 2010, 2005, 2006, 2007, 2008, 2009, 2010)\n",
       "starting_salary: scala.collection.immutable.IndexedSeq[Double] = Vector(2561.95, 4634.2, 4226.06, 4277.46, 4184.32)\n"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val random = new scala.util.Random(123)\n",
    "val years = 6\n",
    "val names = List(\"Alice\", \"Betty\", \"Chris\", \"Dan\", \"Greg\")\n",
    "val names_count = names.length\n",
    "\n",
    "val names_list = (1 to years).flatMap(x => names.map(y => y)).sorted\n",
    "\n",
    "val year = (1 to names_count).flatMap(x => (0 until years).map(y => 2005 + y))\n",
    "\n",
    "val starting_salary = (1 to names_count).map(x => {val n = random.nextGaussian()*1000 + 4000; \n",
    "                                                   BigDecimal(n).setScale(2, BigDecimal.RoundingMode.HALF_UP\n",
    "                                                                         ).toDouble})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temp_salary: List[Double] = List(2561.95, 0.0, 0.0, 0.0, 0.0, 0.0, 4634.2, 0.0, 0.0, 0.0, 0.0, 0.0, 4226.06, 0.0, 0.0, 0.0, 0.0, 0.0, 4277.46, 0.0, 0.0, 0.0, 0.0, 0.0, 4184.32, 0.0, 0.0, 0.0, 0.0, 0.0)\n"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var temp_salary = starting_salary.flatMap(x => x :: List.fill(years-1)(0.0)).toList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "replaceZero: (l: List[Double])List[Double]\n",
       "roundToTwoDec: (l: List[Double])List[Double]\n",
       "zipMany: [X, Y, Z](xs: Seq[X], ys: Seq[Y], zs: Seq[Z])Seq[(X, Y, Z)]\n"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replaceZero(l: List[Double]): List[Double]= {\n",
    "    var diff = random.nextGaussian()*0.09 + 0.1;\n",
    "    l match {\n",
    "      case x :: y :: tail if x != 0.0 & y !=0 => x :: replaceZero(y :: tail)\n",
    "      case x :: 0.0 :: tail if x != 0.0 => x :: replaceZero((x * (1+diff)):: tail)\n",
    "      case x :: Nil  if x != 0.0 => x :: Nil\n",
    "    }\n",
    "}\n",
    "\n",
    "def roundToTwoDec(l: List[Double]) = \n",
    "            l.map(x => BigDecimal(x).setScale(2, BigDecimal.RoundingMode.HALF_UP).toDouble)\n",
    "\n",
    "def zipMany[X,Y,Z](xs: Seq[X], ys: Seq[Y], zs: Seq[Z]): Seq[(X,Y,Z)] =\n",
    "    for (((x, y), z) <- xs zip ys zip zs) yield (x, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// diffrent approach with mutation of the collection\n",
    "\n",
    "// for (    \n",
    "//     n <- 0 until names_count;\n",
    "//     y <- 0 until (years-1)\n",
    "// ){\n",
    "//     var x = random.nextGaussian()*0.09 + 0.1;\n",
    "//     var index = (years*n +1) +y\n",
    "    \n",
    "//     temp_salary(index) =  temp_salary(index - 1) +100\n",
    "// }\n",
    "// temp_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "salary: List[Double] = List(2561.95, 2891.88, 3557.24, 4582.31, 5250.59, 5489.16, 4634.2, 4805.67, 4894.54, 5684.75, 6600.54, 6628.51, 4226.06, 3986.92, 3674.72, 3767.04, 3840.6, 4401.84, 4277.46, 4685.72, 5193.04, 6069.9, 7135.37, 8856.8, 4184.32, 4172.95, 4637.61, 4875.82, 5077.4, 5630.17)\n"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val salary = roundToTwoDec(replaceZero(temp_salary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [name: string, year: int ... 1 more field]\n"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = zipMany(names_list, year, salary).toDF(\"name\", \"year\", \"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------+\n",
      "| name|year| salary|\n",
      "+-----+----+-------+\n",
      "|Alice|2005|2561.95|\n",
      "|Alice|2006|2891.88|\n",
      "|Alice|2007|3557.24|\n",
      "|Alice|2008|4582.31|\n",
      "|Alice|2009|5250.59|\n",
      "|Alice|2010|5489.16|\n",
      "|Betty|2005| 4634.2|\n",
      "|Betty|2006|4805.67|\n",
      "|Betty|2007|4894.54|\n",
      "|Betty|2008|5684.75|\n",
      "|Betty|2009|6600.54|\n",
      "|Betty|2010|6628.51|\n",
      "|Chris|2005|4226.06|\n",
      "|Chris|2006|3986.92|\n",
      "|Chris|2007|3674.72|\n",
      "|Chris|2008|3767.04|\n",
      "|Chris|2009| 3840.6|\n",
      "|Chris|2010|4401.84|\n",
      "|  Dan|2005|4277.46|\n",
      "|  Dan|2006|4685.72|\n",
      "+-----+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------+\n",
      "| name|year| salary|\n",
      "+-----+----+-------+\n",
      "| Greg|2010|7160.96|\n",
      "|  Dan|2010|6777.12|\n",
      "|  Dan|2009|6253.18|\n",
      "| Greg|2009|6139.79|\n",
      "|Betty|2010|5854.52|\n",
      "|Chris|2010|5653.53|\n",
      "| Greg|2008|5602.12|\n",
      "|Chris|2009|5495.22|\n",
      "|  Dan|2008|5104.51|\n",
      "|Chris|2008|4488.78|\n",
      "|Betty|2009|4447.36|\n",
      "|Betty|2008|4183.86|\n",
      "+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"name\") =!= \"Alice\" && col(\"year\") > 2007).orderBy(col(\"salary\").desc).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
